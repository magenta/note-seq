<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>note_seq.audio_io API documentation</title>
<meta name="description" content="Audio file helper functions." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>note_seq.audio_io</code></h1>
</header>
<section id="section-intro">
<p>Audio file helper functions.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright 2021 The Magenta Authors.
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

&#34;&#34;&#34;Audio file helper functions.&#34;&#34;&#34;

import io
import math
import tempfile

import librosa
import numpy as np
import pydub
import scipy.io.wavfile


class AudioIOError(BaseException):  # pylint:disable=g-bad-exception-name
  pass


class AudioIOReadError(AudioIOError):
  pass


class AudioIODataTypeError(AudioIOError):
  pass


def int16_samples_to_float32(y):
  &#34;&#34;&#34;Convert int16 numpy array of audio samples to float32.&#34;&#34;&#34;
  if y.dtype != np.int16:
    raise ValueError(&#39;input samples not int16&#39;)
  return y.astype(np.float32) / np.iinfo(np.int16).max


def float_samples_to_int16(y):
  &#34;&#34;&#34;Convert floating-point numpy array of audio samples to int16.&#34;&#34;&#34;
  if not issubclass(y.dtype.type, np.floating):
    raise ValueError(&#39;input samples not floating-point&#39;)
  return (y * np.iinfo(np.int16).max).astype(np.int16)


def wav_data_to_samples_pydub(wav_data: bytes,
                              sample_rate: int,
                              remove_dc_bias: bool = False,
                              num_channels: int = None,
                              normalize_db: float = None):
  &#34;&#34;&#34;Convert audio file data (in bytes) into a numpy array using Pydub.

  Args:
    wav_data: A byte stream of audio data.
    sample_rate: Resample recorded audio to this sample rate.
    remove_dc_bias: If true, will remove DC bias from audio.
    num_channels: If not specified, output shape will be based on the contents
      of wav_data. Otherwise, will force to be 1 or 2 channels.
    normalize_db: Normalize the audio to this many decibels. Set to None to skip
      normalization step.

  Returns:
    An array of the recorded audio at sample_rate. If mono, will be shape
    [samples], otherwise [channels, samples].
  &#34;&#34;&#34;
  # Parse and normalize the audio.
  aseg = pydub.AudioSegment.from_file(io.BytesIO(wav_data))
  if num_channels:
    aseg = aseg.set_channels(num_channels)
  if remove_dc_bias:
    aseg = aseg.remove_dc_offset()
  if normalize_db is not None:
    aseg.normalize(headroom=normalize_db)
  aseg = aseg.set_frame_rate(sample_rate)

  # Convert to numpy array.
  channel_asegs = aseg.split_to_mono()
  samples = [s.get_array_of_samples() for s in channel_asegs]
  fp_arr = np.array(samples).astype(np.float32)
  fp_arr /= np.iinfo(samples[0].typecode).max

  # If only 1 channel, remove extra dim.
  if fp_arr.shape[0] == 1:
    fp_arr = fp_arr[0]

  return fp_arr


def wav_data_to_samples(wav_data, sample_rate):
  &#34;&#34;&#34;Read PCM-formatted WAV data and return a NumPy array of samples.

  Uses scipy to read and librosa to process WAV data. Audio will be converted to
  mono if necessary.

  Args:
    wav_data: WAV audio data to read.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadError: If scipy is unable to read the WAV data.
    AudioIOError: If audio processing fails.
  &#34;&#34;&#34;
  try:
    # Read the wav file, converting sample rate &amp; number of channels.
    native_sr, y = scipy.io.wavfile.read(io.BytesIO(wav_data))
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOReadError(e)

  if y.dtype == np.int16:
    # Convert to float32.
    y = int16_samples_to_float32(y)
  elif y.dtype == np.float32:
    # Already float32.
    pass
  else:
    raise AudioIOError(
        &#39;WAV file not 16-bit or 32-bit float PCM, unsupported&#39;)
  try:
    # Convert to mono and the desired sample rate.
    if y.ndim == 2 and y.shape[1] == 2:
      y = y.T
      y = librosa.to_mono(y)
    if native_sr != sample_rate:
      y = librosa.resample(y, native_sr, sample_rate)
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOError(e)
  return y


def wav_data_to_samples_librosa(audio_file, sample_rate):
  &#34;&#34;&#34;Loads an in-memory audio file with librosa.

  Use this instead of wav_data_to_samples if the wav is 24-bit, as that&#39;s
  incompatible with wav_data_to_samples internal scipy call.

  Will copy to a local temp file before loading so that librosa can read a file
  path. Librosa does not currently read in-memory files.

  It will be treated as a .wav file.

  Args:
    audio_file: Wav file to load.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadException: If librosa is unable to load the audio data.
  &#34;&#34;&#34;
  with tempfile.NamedTemporaryFile(suffix=&#39;.wav&#39;) as wav_input_file:
    wav_input_file.write(audio_file)
    # Before copying the file, flush any contents
    wav_input_file.flush()
    # And back the file position to top (not need for Copy but for certainty)
    wav_input_file.seek(0)
    return load_audio(wav_input_file.name, sample_rate)


def samples_to_wav_data(samples, sample_rate):
  &#34;&#34;&#34;Converts floating point samples to wav data.&#34;&#34;&#34;
  wav_io = io.BytesIO()
  scipy.io.wavfile.write(wav_io, sample_rate, float_samples_to_int16(samples))
  return wav_io.getvalue()


def crop_samples(samples, sample_rate, crop_beginning_seconds,
                 total_length_seconds):
  &#34;&#34;&#34;Crop WAV data.

  Args:
    samples: Numpy Array containing samples.
    sample_rate: The sample rate at which to interpret the samples.
    crop_beginning_seconds: How many seconds to crop from the beginning of the
        audio.
    total_length_seconds: The desired duration of the audio. After cropping the
        beginning of the audio, any audio longer than this value will be
        deleted.

  Returns:
    A cropped version of the samples.
  &#34;&#34;&#34;
  samples_to_crop = int(crop_beginning_seconds * sample_rate)
  total_samples = int(total_length_seconds * sample_rate)
  cropped_samples = samples[samples_to_crop:(samples_to_crop + total_samples)]
  return cropped_samples


def repeat_samples_to_duration(samples, sample_rate, duration):
  &#34;&#34;&#34;Repeat a sequence of samples until it is a given duration, trimming extra.

  Args:
    samples: The sequence to repeat
    sample_rate: The sample rate at which to interpret the samples.
    duration: The desired duration

  Returns:
    The repeated and possibly trimmed sequence.
  &#34;&#34;&#34;
  sequence_duration = len(samples) / sample_rate
  num_repeats = int(math.ceil(duration / sequence_duration))
  repeated_samples = np.concatenate([samples] * num_repeats)
  trimmed = crop_samples(
      repeated_samples, sample_rate,
      crop_beginning_seconds=0, total_length_seconds=duration)
  return trimmed


def crop_wav_data(wav_data, sample_rate, crop_beginning_seconds,
                  total_length_seconds):
  &#34;&#34;&#34;Crop WAV data.

  Args:
    wav_data: WAV audio data to crop.
    sample_rate: The sample rate at which to read the WAV data.
    crop_beginning_seconds: How many seconds to crop from the beginning of the
        audio.
    total_length_seconds: The desired duration of the audio. After cropping the
        beginning of the audio, any audio longer than this value will be
        deleted.

  Returns:
    A cropped version of the WAV audio.
  &#34;&#34;&#34;
  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  samples_to_crop = int(crop_beginning_seconds * sample_rate)
  total_samples = int(total_length_seconds * sample_rate)
  cropped_samples = y[samples_to_crop:(samples_to_crop + total_samples)]
  return samples_to_wav_data(cropped_samples, sample_rate)


def jitter_wav_data(wav_data, sample_rate, jitter_seconds):
  &#34;&#34;&#34;Add silence to the beginning of the file.

  Args:
     wav_data: WAV audio data to prepend with silence.
     sample_rate: The sample rate at which to read the WAV data.
     jitter_seconds: Seconds of silence to prepend.

  Returns:
     A version of the WAV audio with jitter_seconds silence prepended.
  &#34;&#34;&#34;

  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  silence_samples = jitter_seconds * sample_rate
  new_y = np.concatenate((np.zeros(np.int(silence_samples)), y))
  return samples_to_wav_data(new_y, sample_rate)


def load_audio(audio_filename, sample_rate):
  &#34;&#34;&#34;Loads an audio file.

  Args:
    audio_filename: File path to load.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadError: If librosa is unable to load the audio data.
  &#34;&#34;&#34;
  try:
    y, unused_sr = librosa.load(audio_filename, sr=sample_rate, mono=True)
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOReadError(e)
  return y


def make_stereo(left, right):
  &#34;&#34;&#34;Combine two mono signals into one stereo signal.

  Both signals must have the same data type. The resulting track will be the
  length of the longer of the two signals.

  Args:
    left: Samples for the left channel.
    right: Samples for the right channel.

  Returns:
    The two channels combined into a stereo signal.

  Raises:
    AudioIODataTypeError: if the two signals have different data types.
  &#34;&#34;&#34;
  if left.dtype != right.dtype:
    raise AudioIODataTypeError(
        &#39;left channel is of type {}, but right channel is {}&#39;.format(
            left.dtype, right.dtype))

  # Mask of valid places in each row
  lens = np.array([len(left), len(right)])
  mask = np.arange(lens.max()) &lt; lens[:, None]

  # Setup output array and put elements from data into masked positions
  out = np.zeros(mask.shape, dtype=left.dtype)
  out[mask] = np.concatenate([left, right])
  return out.T


def normalize_wav_data(wav_data, sample_rate, norm=np.inf):
  &#34;&#34;&#34;Normalizes wav data.

  Args:
     wav_data: WAV audio data to prepend with silence.
     sample_rate: The sample rate at which to read the WAV data.
     norm: See the norm argument of librosa.util.normalize.

  Returns:
     A version of the WAV audio that has been normalized.
  &#34;&#34;&#34;

  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  new_y = librosa.util.normalize(y, norm=norm)
  return samples_to_wav_data(new_y, sample_rate)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="note_seq.audio_io.crop_samples"><code class="name flex">
<span>def <span class="ident">crop_samples</span></span>(<span>samples, sample_rate, crop_beginning_seconds, total_length_seconds)</span>
</code></dt>
<dd>
<div class="desc"><p>Crop WAV data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>samples</code></strong></dt>
<dd>Numpy Array containing samples.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate at which to interpret the samples.</dd>
<dt><strong><code>crop_beginning_seconds</code></strong></dt>
<dd>How many seconds to crop from the beginning of the
audio.</dd>
<dt><strong><code>total_length_seconds</code></strong></dt>
<dd>The desired duration of the audio. After cropping the
beginning of the audio, any audio longer than this value will be
deleted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A cropped version of the samples.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crop_samples(samples, sample_rate, crop_beginning_seconds,
                 total_length_seconds):
  &#34;&#34;&#34;Crop WAV data.

  Args:
    samples: Numpy Array containing samples.
    sample_rate: The sample rate at which to interpret the samples.
    crop_beginning_seconds: How many seconds to crop from the beginning of the
        audio.
    total_length_seconds: The desired duration of the audio. After cropping the
        beginning of the audio, any audio longer than this value will be
        deleted.

  Returns:
    A cropped version of the samples.
  &#34;&#34;&#34;
  samples_to_crop = int(crop_beginning_seconds * sample_rate)
  total_samples = int(total_length_seconds * sample_rate)
  cropped_samples = samples[samples_to_crop:(samples_to_crop + total_samples)]
  return cropped_samples</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.crop_wav_data"><code class="name flex">
<span>def <span class="ident">crop_wav_data</span></span>(<span>wav_data, sample_rate, crop_beginning_seconds, total_length_seconds)</span>
</code></dt>
<dd>
<div class="desc"><p>Crop WAV data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wav_data</code></strong></dt>
<dd>WAV audio data to crop.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate at which to read the WAV data.</dd>
<dt><strong><code>crop_beginning_seconds</code></strong></dt>
<dd>How many seconds to crop from the beginning of the
audio.</dd>
<dt><strong><code>total_length_seconds</code></strong></dt>
<dd>The desired duration of the audio. After cropping the
beginning of the audio, any audio longer than this value will be
deleted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A cropped version of the WAV audio.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crop_wav_data(wav_data, sample_rate, crop_beginning_seconds,
                  total_length_seconds):
  &#34;&#34;&#34;Crop WAV data.

  Args:
    wav_data: WAV audio data to crop.
    sample_rate: The sample rate at which to read the WAV data.
    crop_beginning_seconds: How many seconds to crop from the beginning of the
        audio.
    total_length_seconds: The desired duration of the audio. After cropping the
        beginning of the audio, any audio longer than this value will be
        deleted.

  Returns:
    A cropped version of the WAV audio.
  &#34;&#34;&#34;
  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  samples_to_crop = int(crop_beginning_seconds * sample_rate)
  total_samples = int(total_length_seconds * sample_rate)
  cropped_samples = y[samples_to_crop:(samples_to_crop + total_samples)]
  return samples_to_wav_data(cropped_samples, sample_rate)</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.float_samples_to_int16"><code class="name flex">
<span>def <span class="ident">float_samples_to_int16</span></span>(<span>y)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert floating-point numpy array of audio samples to int16.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def float_samples_to_int16(y):
  &#34;&#34;&#34;Convert floating-point numpy array of audio samples to int16.&#34;&#34;&#34;
  if not issubclass(y.dtype.type, np.floating):
    raise ValueError(&#39;input samples not floating-point&#39;)
  return (y * np.iinfo(np.int16).max).astype(np.int16)</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.int16_samples_to_float32"><code class="name flex">
<span>def <span class="ident">int16_samples_to_float32</span></span>(<span>y)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert int16 numpy array of audio samples to float32.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def int16_samples_to_float32(y):
  &#34;&#34;&#34;Convert int16 numpy array of audio samples to float32.&#34;&#34;&#34;
  if y.dtype != np.int16:
    raise ValueError(&#39;input samples not int16&#39;)
  return y.astype(np.float32) / np.iinfo(np.int16).max</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.jitter_wav_data"><code class="name flex">
<span>def <span class="ident">jitter_wav_data</span></span>(<span>wav_data, sample_rate, jitter_seconds)</span>
</code></dt>
<dd>
<div class="desc"><p>Add silence to the beginning of the file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wav_data</code></strong></dt>
<dd>WAV audio data to prepend with silence.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate at which to read the WAV data.</dd>
<dt><strong><code>jitter_seconds</code></strong></dt>
<dd>Seconds of silence to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A version of the WAV audio with jitter_seconds silence prepended.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jitter_wav_data(wav_data, sample_rate, jitter_seconds):
  &#34;&#34;&#34;Add silence to the beginning of the file.

  Args:
     wav_data: WAV audio data to prepend with silence.
     sample_rate: The sample rate at which to read the WAV data.
     jitter_seconds: Seconds of silence to prepend.

  Returns:
     A version of the WAV audio with jitter_seconds silence prepended.
  &#34;&#34;&#34;

  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  silence_samples = jitter_seconds * sample_rate
  new_y = np.concatenate((np.zeros(np.int(silence_samples)), y))
  return samples_to_wav_data(new_y, sample_rate)</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.load_audio"><code class="name flex">
<span>def <span class="ident">load_audio</span></span>(<span>audio_filename, sample_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads an audio file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>audio_filename</code></strong></dt>
<dd>File path to load.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The number of samples per second at which the audio will be
returned. Resampling will be performed if necessary.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A numpy array of audio samples, single-channel (mono) and sampled at the
specified rate, in float32 format.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="note_seq.audio_io.AudioIOReadError" href="#note_seq.audio_io.AudioIOReadError">AudioIOReadError</a></code></dt>
<dd>If librosa is unable to load the audio data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_audio(audio_filename, sample_rate):
  &#34;&#34;&#34;Loads an audio file.

  Args:
    audio_filename: File path to load.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadError: If librosa is unable to load the audio data.
  &#34;&#34;&#34;
  try:
    y, unused_sr = librosa.load(audio_filename, sr=sample_rate, mono=True)
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOReadError(e)
  return y</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.make_stereo"><code class="name flex">
<span>def <span class="ident">make_stereo</span></span>(<span>left, right)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine two mono signals into one stereo signal.</p>
<p>Both signals must have the same data type. The resulting track will be the
length of the longer of the two signals.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>left</code></strong></dt>
<dd>Samples for the left channel.</dd>
<dt><strong><code>right</code></strong></dt>
<dd>Samples for the right channel.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The two channels combined into a stereo signal.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="note_seq.audio_io.AudioIODataTypeError" href="#note_seq.audio_io.AudioIODataTypeError">AudioIODataTypeError</a></code></dt>
<dd>if the two signals have different data types.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_stereo(left, right):
  &#34;&#34;&#34;Combine two mono signals into one stereo signal.

  Both signals must have the same data type. The resulting track will be the
  length of the longer of the two signals.

  Args:
    left: Samples for the left channel.
    right: Samples for the right channel.

  Returns:
    The two channels combined into a stereo signal.

  Raises:
    AudioIODataTypeError: if the two signals have different data types.
  &#34;&#34;&#34;
  if left.dtype != right.dtype:
    raise AudioIODataTypeError(
        &#39;left channel is of type {}, but right channel is {}&#39;.format(
            left.dtype, right.dtype))

  # Mask of valid places in each row
  lens = np.array([len(left), len(right)])
  mask = np.arange(lens.max()) &lt; lens[:, None]

  # Setup output array and put elements from data into masked positions
  out = np.zeros(mask.shape, dtype=left.dtype)
  out[mask] = np.concatenate([left, right])
  return out.T</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.normalize_wav_data"><code class="name flex">
<span>def <span class="ident">normalize_wav_data</span></span>(<span>wav_data, sample_rate, norm=inf)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes wav data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wav_data</code></strong></dt>
<dd>WAV audio data to prepend with silence.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate at which to read the WAV data.</dd>
<dt><strong><code>norm</code></strong></dt>
<dd>See the norm argument of librosa.util.normalize.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A version of the WAV audio that has been normalized.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_wav_data(wav_data, sample_rate, norm=np.inf):
  &#34;&#34;&#34;Normalizes wav data.

  Args:
     wav_data: WAV audio data to prepend with silence.
     sample_rate: The sample rate at which to read the WAV data.
     norm: See the norm argument of librosa.util.normalize.

  Returns:
     A version of the WAV audio that has been normalized.
  &#34;&#34;&#34;

  y = wav_data_to_samples(wav_data, sample_rate=sample_rate)
  new_y = librosa.util.normalize(y, norm=norm)
  return samples_to_wav_data(new_y, sample_rate)</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.repeat_samples_to_duration"><code class="name flex">
<span>def <span class="ident">repeat_samples_to_duration</span></span>(<span>samples, sample_rate, duration)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeat a sequence of samples until it is a given duration, trimming extra.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>samples</code></strong></dt>
<dd>The sequence to repeat</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The sample rate at which to interpret the samples.</dd>
<dt><strong><code>duration</code></strong></dt>
<dd>The desired duration</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The repeated and possibly trimmed sequence.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def repeat_samples_to_duration(samples, sample_rate, duration):
  &#34;&#34;&#34;Repeat a sequence of samples until it is a given duration, trimming extra.

  Args:
    samples: The sequence to repeat
    sample_rate: The sample rate at which to interpret the samples.
    duration: The desired duration

  Returns:
    The repeated and possibly trimmed sequence.
  &#34;&#34;&#34;
  sequence_duration = len(samples) / sample_rate
  num_repeats = int(math.ceil(duration / sequence_duration))
  repeated_samples = np.concatenate([samples] * num_repeats)
  trimmed = crop_samples(
      repeated_samples, sample_rate,
      crop_beginning_seconds=0, total_length_seconds=duration)
  return trimmed</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.samples_to_wav_data"><code class="name flex">
<span>def <span class="ident">samples_to_wav_data</span></span>(<span>samples, sample_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts floating point samples to wav data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def samples_to_wav_data(samples, sample_rate):
  &#34;&#34;&#34;Converts floating point samples to wav data.&#34;&#34;&#34;
  wav_io = io.BytesIO()
  scipy.io.wavfile.write(wav_io, sample_rate, float_samples_to_int16(samples))
  return wav_io.getvalue()</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.wav_data_to_samples"><code class="name flex">
<span>def <span class="ident">wav_data_to_samples</span></span>(<span>wav_data, sample_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Read PCM-formatted WAV data and return a NumPy array of samples.</p>
<p>Uses scipy to read and librosa to process WAV data. Audio will be converted to
mono if necessary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wav_data</code></strong></dt>
<dd>WAV audio data to read.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The number of samples per second at which the audio will be
returned. Resampling will be performed if necessary.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A numpy array of audio samples, single-channel (mono) and sampled at the
specified rate, in float32 format.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="note_seq.audio_io.AudioIOReadError" href="#note_seq.audio_io.AudioIOReadError">AudioIOReadError</a></code></dt>
<dd>If scipy is unable to read the WAV data.</dd>
<dt><code><a title="note_seq.audio_io.AudioIOError" href="#note_seq.audio_io.AudioIOError">AudioIOError</a></code></dt>
<dd>If audio processing fails.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wav_data_to_samples(wav_data, sample_rate):
  &#34;&#34;&#34;Read PCM-formatted WAV data and return a NumPy array of samples.

  Uses scipy to read and librosa to process WAV data. Audio will be converted to
  mono if necessary.

  Args:
    wav_data: WAV audio data to read.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadError: If scipy is unable to read the WAV data.
    AudioIOError: If audio processing fails.
  &#34;&#34;&#34;
  try:
    # Read the wav file, converting sample rate &amp; number of channels.
    native_sr, y = scipy.io.wavfile.read(io.BytesIO(wav_data))
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOReadError(e)

  if y.dtype == np.int16:
    # Convert to float32.
    y = int16_samples_to_float32(y)
  elif y.dtype == np.float32:
    # Already float32.
    pass
  else:
    raise AudioIOError(
        &#39;WAV file not 16-bit or 32-bit float PCM, unsupported&#39;)
  try:
    # Convert to mono and the desired sample rate.
    if y.ndim == 2 and y.shape[1] == 2:
      y = y.T
      y = librosa.to_mono(y)
    if native_sr != sample_rate:
      y = librosa.resample(y, native_sr, sample_rate)
  except Exception as e:  # pylint: disable=broad-except
    raise AudioIOError(e)
  return y</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.wav_data_to_samples_librosa"><code class="name flex">
<span>def <span class="ident">wav_data_to_samples_librosa</span></span>(<span>audio_file, sample_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads an in-memory audio file with librosa.</p>
<p>Use this instead of wav_data_to_samples if the wav is 24-bit, as that's
incompatible with wav_data_to_samples internal scipy call.</p>
<p>Will copy to a local temp file before loading so that librosa can read a file
path. Librosa does not currently read in-memory files.</p>
<p>It will be treated as a .wav file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>audio_file</code></strong></dt>
<dd>Wav file to load.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>The number of samples per second at which the audio will be
returned. Resampling will be performed if necessary.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A numpy array of audio samples, single-channel (mono) and sampled at the
specified rate, in float32 format.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AudioIOReadException</code></dt>
<dd>If librosa is unable to load the audio data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wav_data_to_samples_librosa(audio_file, sample_rate):
  &#34;&#34;&#34;Loads an in-memory audio file with librosa.

  Use this instead of wav_data_to_samples if the wav is 24-bit, as that&#39;s
  incompatible with wav_data_to_samples internal scipy call.

  Will copy to a local temp file before loading so that librosa can read a file
  path. Librosa does not currently read in-memory files.

  It will be treated as a .wav file.

  Args:
    audio_file: Wav file to load.
    sample_rate: The number of samples per second at which the audio will be
        returned. Resampling will be performed if necessary.

  Returns:
    A numpy array of audio samples, single-channel (mono) and sampled at the
    specified rate, in float32 format.

  Raises:
    AudioIOReadException: If librosa is unable to load the audio data.
  &#34;&#34;&#34;
  with tempfile.NamedTemporaryFile(suffix=&#39;.wav&#39;) as wav_input_file:
    wav_input_file.write(audio_file)
    # Before copying the file, flush any contents
    wav_input_file.flush()
    # And back the file position to top (not need for Copy but for certainty)
    wav_input_file.seek(0)
    return load_audio(wav_input_file.name, sample_rate)</code></pre>
</details>
</dd>
<dt id="note_seq.audio_io.wav_data_to_samples_pydub"><code class="name flex">
<span>def <span class="ident">wav_data_to_samples_pydub</span></span>(<span>wav_data: bytes, sample_rate: int, remove_dc_bias: bool = False, num_channels: int = None, normalize_db: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert audio file data (in bytes) into a numpy array using Pydub.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wav_data</code></strong></dt>
<dd>A byte stream of audio data.</dd>
<dt><strong><code>sample_rate</code></strong></dt>
<dd>Resample recorded audio to this sample rate.</dd>
<dt><strong><code>remove_dc_bias</code></strong></dt>
<dd>If true, will remove DC bias from audio.</dd>
<dt><strong><code>num_channels</code></strong></dt>
<dd>If not specified, output shape will be based on the contents
of wav_data. Otherwise, will force to be 1 or 2 channels.</dd>
<dt><strong><code>normalize_db</code></strong></dt>
<dd>Normalize the audio to this many decibels. Set to None to skip
normalization step.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An array of the recorded audio at sample_rate. If mono, will be shape
[samples], otherwise [channels, samples].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wav_data_to_samples_pydub(wav_data: bytes,
                              sample_rate: int,
                              remove_dc_bias: bool = False,
                              num_channels: int = None,
                              normalize_db: float = None):
  &#34;&#34;&#34;Convert audio file data (in bytes) into a numpy array using Pydub.

  Args:
    wav_data: A byte stream of audio data.
    sample_rate: Resample recorded audio to this sample rate.
    remove_dc_bias: If true, will remove DC bias from audio.
    num_channels: If not specified, output shape will be based on the contents
      of wav_data. Otherwise, will force to be 1 or 2 channels.
    normalize_db: Normalize the audio to this many decibels. Set to None to skip
      normalization step.

  Returns:
    An array of the recorded audio at sample_rate. If mono, will be shape
    [samples], otherwise [channels, samples].
  &#34;&#34;&#34;
  # Parse and normalize the audio.
  aseg = pydub.AudioSegment.from_file(io.BytesIO(wav_data))
  if num_channels:
    aseg = aseg.set_channels(num_channels)
  if remove_dc_bias:
    aseg = aseg.remove_dc_offset()
  if normalize_db is not None:
    aseg.normalize(headroom=normalize_db)
  aseg = aseg.set_frame_rate(sample_rate)

  # Convert to numpy array.
  channel_asegs = aseg.split_to_mono()
  samples = [s.get_array_of_samples() for s in channel_asegs]
  fp_arr = np.array(samples).astype(np.float32)
  fp_arr /= np.iinfo(samples[0].typecode).max

  # If only 1 channel, remove extra dim.
  if fp_arr.shape[0] == 1:
    fp_arr = fp_arr[0]

  return fp_arr</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="note_seq.audio_io.AudioIODataTypeError"><code class="flex name class">
<span>class <span class="ident">AudioIODataTypeError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all exceptions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioIODataTypeError(AudioIOError):
  pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="note_seq.audio_io.AudioIOError" href="#note_seq.audio_io.AudioIOError">AudioIOError</a></li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="note_seq.audio_io.AudioIOError"><code class="flex name class">
<span>class <span class="ident">AudioIOError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all exceptions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioIOError(BaseException):  # pylint:disable=g-bad-exception-name
  pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="note_seq.audio_io.AudioIODataTypeError" href="#note_seq.audio_io.AudioIODataTypeError">AudioIODataTypeError</a></li>
<li><a title="note_seq.audio_io.AudioIOReadError" href="#note_seq.audio_io.AudioIOReadError">AudioIOReadError</a></li>
</ul>
</dd>
<dt id="note_seq.audio_io.AudioIOReadError"><code class="flex name class">
<span>class <span class="ident">AudioIOReadError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all exceptions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioIOReadError(AudioIOError):
  pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="note_seq.audio_io.AudioIOError" href="#note_seq.audio_io.AudioIOError">AudioIOError</a></li>
<li>builtins.BaseException</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="note_seq" href="index.html">note_seq</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="note_seq.audio_io.crop_samples" href="#note_seq.audio_io.crop_samples">crop_samples</a></code></li>
<li><code><a title="note_seq.audio_io.crop_wav_data" href="#note_seq.audio_io.crop_wav_data">crop_wav_data</a></code></li>
<li><code><a title="note_seq.audio_io.float_samples_to_int16" href="#note_seq.audio_io.float_samples_to_int16">float_samples_to_int16</a></code></li>
<li><code><a title="note_seq.audio_io.int16_samples_to_float32" href="#note_seq.audio_io.int16_samples_to_float32">int16_samples_to_float32</a></code></li>
<li><code><a title="note_seq.audio_io.jitter_wav_data" href="#note_seq.audio_io.jitter_wav_data">jitter_wav_data</a></code></li>
<li><code><a title="note_seq.audio_io.load_audio" href="#note_seq.audio_io.load_audio">load_audio</a></code></li>
<li><code><a title="note_seq.audio_io.make_stereo" href="#note_seq.audio_io.make_stereo">make_stereo</a></code></li>
<li><code><a title="note_seq.audio_io.normalize_wav_data" href="#note_seq.audio_io.normalize_wav_data">normalize_wav_data</a></code></li>
<li><code><a title="note_seq.audio_io.repeat_samples_to_duration" href="#note_seq.audio_io.repeat_samples_to_duration">repeat_samples_to_duration</a></code></li>
<li><code><a title="note_seq.audio_io.samples_to_wav_data" href="#note_seq.audio_io.samples_to_wav_data">samples_to_wav_data</a></code></li>
<li><code><a title="note_seq.audio_io.wav_data_to_samples" href="#note_seq.audio_io.wav_data_to_samples">wav_data_to_samples</a></code></li>
<li><code><a title="note_seq.audio_io.wav_data_to_samples_librosa" href="#note_seq.audio_io.wav_data_to_samples_librosa">wav_data_to_samples_librosa</a></code></li>
<li><code><a title="note_seq.audio_io.wav_data_to_samples_pydub" href="#note_seq.audio_io.wav_data_to_samples_pydub">wav_data_to_samples_pydub</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="note_seq.audio_io.AudioIODataTypeError" href="#note_seq.audio_io.AudioIODataTypeError">AudioIODataTypeError</a></code></h4>
</li>
<li>
<h4><code><a title="note_seq.audio_io.AudioIOError" href="#note_seq.audio_io.AudioIOError">AudioIOError</a></code></h4>
</li>
<li>
<h4><code><a title="note_seq.audio_io.AudioIOReadError" href="#note_seq.audio_io.AudioIOReadError">AudioIOReadError</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>